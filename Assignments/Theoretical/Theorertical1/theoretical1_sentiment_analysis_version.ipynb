{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGmLTuII-aAh"
      },
      "source": [
        "# Tweet classification with naive bayes\n",
        "\n",
        "For this notebook we are going to implement a naive bayes classifier for classifying positive or negative based on the words in the tweet. Recall that for two events A and B the bayes theorem says\n",
        "\n",
        "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
        "\n",
        "where P(A) and P(B) is the ***class probabilities*** and P(B|A) is called ***conditional probabilities***. this gives us the probability of A happening, given that B has occurred. So as an example if we want to find the probability of \"is this a positive tweet given that it contains the word \"good\" \" we will obtain the following\n",
        "\n",
        "$$ P(\\text{``positive\"}|\\text{``good\" in tweet}) = \\frac{P(\\text{``good\" in tweet}|\\text{``positive\"})P(\\text{``positive\"})}{P(\\text{``good\" in tweet})} $$\n",
        "\n",
        "This means that to find the probability of \"is this a positive tweet given that it contains the word \"good\" \" we need the probability of \"good\" being in a positive tweet, the probability of a tweet being positive and the probability of \"good\" being in a tweet.\n",
        "\n",
        "Similarly, if we want to obtain the opposite \"is this a negative tweet given that it contains the word \"boring\" \"\n",
        "we get\n",
        "\n",
        "$$ P(\\text{``negative\"}|\\text{``boring\" in tweet}) = \\frac{P(\\text{``boring\" in tweet}|\\text{``negative\"})P(\\text{``negative\"})}{P(\\text{``boring\" in tweet})} $$\n",
        "\n",
        "where we need the probability of \"boring\" being in a negative tweet, the probability of a tweet negative being and the probability of \"boring\" being in a tweet.\n",
        "\n",
        "We can now build a classifier where we compare those two probabilities and whichever is the larger one it's classified as\n",
        "\n",
        "if P(\"positive\"|\"good\" in tweet) $>$ P(\"negative\"|\"boring\" in tweet)\n",
        "    \n",
        "   Tweet is positive\n",
        "\n",
        "else\n",
        "   \n",
        "   Tweet is negative\n",
        "\n",
        "Now let's expand this to handle multiple features and put the Naive assumption into bayes theroem. This means that if features are independent we have\n",
        "\n",
        "$$ P(A,B) = P(A)P(B) $$\n",
        "\n",
        "This gives us:\n",
        "\n",
        "$$ P(A|b_1,b_2,...,b_n) = \\frac{P(b_1|A)P(b_2|A)...P(b_n|A)P(A)}{P(b_1)P(b_2)...P(b_n)} $$\n",
        "\n",
        "or\n",
        "\n",
        "$$ P(A|b_1,b_2,...,b_n) = \\frac{\\prod_i^nP(b_i|A)P(A)}{P(b_1)P(b_2)...P(b_n)} $$\n",
        "\n",
        "\n",
        "So with our previous example expanded with more words \"is this a positive tweet given that it contains the word \"good\" and \"interesting\" \" gives us\n",
        "\n",
        "$$ P(\\text{``positive\"}|\\text{``good\", ``interesting\" in tweet}) = \\frac{P(\\text{``good\" in tweet}|\\text{``positive\"})P(\\text{``interesting\" in tweet}|\\text{``positive\"})P(\\text{``positive\"})}{P(\\text{``good\" in tweet})P(\\text{``interesting\" in tweet})} $$\n",
        "\n",
        "As you can see the denominator remains constant which means we can remove it and the final classifier end up\n",
        "\n",
        "$$y = argmax_A P(A)\\prod_i^nP(b_i|A) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A227dex-aAk"
      },
      "source": [
        "The dataset that you will be working with can be downloaded from the following link: https://uppsala.instructure.com/courses/66466/files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T13:37:14.983555600Z",
          "start_time": "2024-01-07T13:37:13.641527800Z"
        },
        "id": "nX86B9UG-aAk"
      },
      "outputs": [],
      "source": [
        "#stuff to import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imNNP9_S-aAl"
      },
      "source": [
        "#### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T13:59:08.449137900Z",
          "start_time": "2024-01-07T13:59:07.974610Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "T61F_B1E-aAl",
        "outputId": "9efa893a-ef52-44df-a2a7-f5f784288450",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data_for_theoretical_notebook_1.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4ad55ef4109c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#caricamento del file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtweets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_for_theoretical_notebook_1.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_for_theoretical_notebook_1.csv'"
          ]
        }
      ],
      "source": [
        "#caricamento del file\n",
        "tweets=pd.read_csv('data_for_theoretical_notebook_1.csv',encoding='latin')\n",
        "tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DweRcNb3-aAl"
      },
      "source": [
        "Now lets split the data into a training set and a test set using scikit-learns train_test_split function\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-01-07T13:37:59.025504100Z",
          "start_time": "2024-01-07T13:37:59.020482200Z"
        },
        "id": "f-0FpeBJ-aAm"
      },
      "outputs": [],
      "source": [
        "#divido in due dataframe i dati\n",
        "tweets_data = tweets[\"processed_tweets\"] #dataframe per i tweet processati (non prendo i tweet originali perchè contengono errori a palate)\n",
        "tweets_labels = tweets[\"sentiment\"] # dataframe con il semtiment (0 se è negativo 1 se è positivo)\n",
        "\n",
        "#splitto ciascuno dei due dataframe appena creati in due in modo da avere dati per il training e dati per il testing\n",
        "# STUDENT CODE HERE\n",
        "train_tweets, test_tweets = train_test_split(tweets_data, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
        "train_labels, test_labels = train_test_split(tweets_labels, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
        "# STUDENT CODE ENDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MkeP6Q5-aAm"
      },
      "source": [
        "What we need to build our classifier is \"probability of positive tweet\" P(pos) , \"probability of negative tweet\" P(neg), \"probability of word in tweet given tweet is positive\" P(w|pos) and \"probability of word in tweet given tweet is negative\" P(w|neg). Start by calculating the probability that a tweet is positive and negative respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoNbTEHo-aAm",
        "outputId": "08551f70-4f99-4add-c27f-cb97bfd3ae44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.49971662010542733\n",
            "0.5002833798945727\n"
          ]
        }
      ],
      "source": [
        "# STUDENT CODE HERE\n",
        "# STEP A) PER L'UTILIZZO DI BAYES\n",
        "P_pos = tweets['sentiment'].mean()\n",
        "P_neg = 1 - P_pos\n",
        "print(P_pos)\n",
        "print(P_neg)\n",
        "# STUDENT CODE ENDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGlG34gi-aAm"
      },
      "source": [
        "For P(w|pos), P(w|neg) we need to count how many tweets each word occur in. Count the number of tweets each word occurs in and store in the word counter. An entry in the word counter is for instance {'good': 'Pos':150, 'Neg': 10} meaning good occurs in 150 positive tweets and 10 negative tweets. Be aware that we are not interested in calculating multiple occurrences of the same word in the same tweet. Also, we change the labels from 0 for \"Negative\" and 1 for \"Positive\" to \"Neg\" and \"Pos\" respectively. For each word convert it to lower case. You can use Python's [lower](https://www.w3schools.com/python/ref_string_lower.asp). Another handy Python string method is [split](https://www.w3schools.com/python/ref_string_split.asp)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-pA_pFm-aAm"
      },
      "outputs": [],
      "source": [
        "#per comodità assegno nei train labels 0 e 1 al rispettivo sentiment\n",
        "new_train_labels = train_labels.replace(0, \"Neg\", regex=True)\n",
        "final_train_labels = new_train_labels.replace(1, \"Pos\", regex=True)\n",
        "\n",
        "    # ... Count number of tweets each word occurs in and store in word_counter where an entry looks like ex. {'word': 'Pos':98, 'Neg':10}\n",
        "    # STUDENT CODE HERE\n",
        "word_counter = {}\n",
        "\n",
        "# per ogni tweet, label (avrei potuto utilizzare qualsiasi altro indice nel ciclo for) presente nei train_tweets e nei train_labels\n",
        "for tweet, label in zip(train_tweets, final_train_labels):\n",
        "\n",
        "    words_in_tweet = set(tweet.lower().split()) #set: rimuove i duplicati  lower: converte in minuscolo #split: divide la stringa (frase completa) in parole se di mezzo c'è uno spazio\n",
        "    for word in words_in_tweet: #il ciclo scorre ogni parola del tweet (sto scorrendo i tweet nel ciclo precedente)\n",
        "        if word not in word_counter:\n",
        "            word_counter[word] = {'Pos': 0, 'Neg': 0} #se la parola non è ancora presente nel word counter la inizializzo con (0,0)\n",
        "        word_counter[word][label] += 1 #mi incrementa il conteggio di 'pos' o 'neg' di 1 il base al label associato al tweet\n",
        "\n",
        "\n",
        "#print(word_counter)\n",
        "\n",
        "\n",
        "\n",
        "    # STUDENT CODE ENDS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KhvKnW3-aAn"
      },
      "source": [
        "Let's work with a smaller subset of words just to save up some time. Find the 1500 most occuring words in tweet data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVhS6R1P-aAn"
      },
      "outputs": [],
      "source": [
        "nr_of_words_to_use = 1500\n",
        "popular_words = sorted(word_counter.items(), key=lambda x: x[1]['Pos'] + x[1]['Neg'], reverse=True)\n",
        "popular_words = [x[0] for x in popular_words[:nr_of_words_to_use]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkX1H0F5-aAn"
      },
      "source": [
        "Now let's compute P(w|pos), P(w|neg) for the popular words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emT5JlEH-aAn"
      },
      "outputs": [],
      "source": [
        "#STEP b) PER L'UTILIZZO DI BAYES\n",
        "P_w_given_pos = {}\n",
        "P_w_given_neg = {}\n",
        "\n",
        "    #  Calculate the two probabilities\n",
        "    # STUDENT CODE HERE\n",
        "# devo calcolare quanto è probabile trovare ogni parola nel subset di tweet positivi e in quello dei negativi\n",
        "tot_neg_tweets,tot_pos_tweets=tweets['sentiment'].value_counts()\n",
        "#print(tot_pos_tweets)\n",
        "#print(tot_neg_tweets)\n",
        "for word in popular_words:\n",
        "    P_w_given_pos[word]=word_counter[word]['Pos']/tot_pos_tweets\n",
        "    P_w_given_neg[word]=word_counter[word]['Neg']/tot_neg_tweets\n",
        "\n",
        "#print(P_w_given_pos)\n",
        "#print(P_w_given_neg)\n",
        "\n",
        "\n",
        "\n",
        "    # STUDENT CODE ENDS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx0wpntT-aAn"
      },
      "outputs": [],
      "source": [
        "classifier = {\n",
        "    'basis'  : popular_words,\n",
        "    'P(pos)'   : P_pos,\n",
        "    'P(neg)'   : P_neg,\n",
        "    'P(w|pos)' : P_w_given_pos,\n",
        "    'P(w|neg)' : P_w_given_neg\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tE60FC2-aAn"
      },
      "source": [
        "#### Train and predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dqjqrqb-aAn"
      },
      "source": [
        "Write a tweet_classifier function that takes your trained classifier and a tweet and returns wether it's about Positive or Negative using the popular words selected. Note that if there are words in the basis words in our classifier that are not in the tweet we have the opposite probabilities i.e P(w_1 occurs )* P(w_2 does not occur) * .... if w_1 occurs and w_2 does not occur. The function should return wether the tweet is Positive or Negative. i.e 'Pos' or 'Neg'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j886uxW6-aAn"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import math\n",
        "\n",
        "def tweet_classifier(tweet, classifier_dict):\n",
        "    \"\"\"\n",
        "    Classifica un tweet come 'Pos' o 'Neg' usando il classificatore Naive Bayes.\n",
        "\n",
        "    param tweet: stringa contenente il tweet da classificare\n",
        "    param classifier_dict: dizionario contenente:\n",
        "        'basis' - lista delle parole più frequenti\n",
        "        'P(pos)' - probabilità a priori di 'Pos'\n",
        "        'P(neg)' - probabilità a priori di 'Neg'\n",
        "        'P(w|pos)' - probabilità condizionata delle parole nei tweet positivi\n",
        "        'P(w|neg)' - probabilità condizionata delle parole nei tweet negativi\n",
        "    return: 'Pos' o 'Neg'\n",
        "    \"\"\"\n",
        "    # Calcolo dei logaritmi delle probabilità a priori (per evitare underflow numerico)\n",
        "    y_pos = math.log(classifier_dict['P(pos)'])\n",
        "    y_neg = math.log(classifier_dict['P(neg)'])\n",
        "\n",
        "    # Estrazione delle parole dal tweet (trasformiamo in minuscolo e togliamo i duplicati)\n",
        "    words_in_tweet = set(tweet.lower().split())\n",
        "\n",
        "    # Calcolo delle probabilità condizionate\n",
        "    for word in words_in_tweet:\n",
        "        if word in classifier_dict['P(w|pos)']:  # Se la parola è conosciuta\n",
        "            y_pos += math.log(classifier_dict['P(w|pos)'][word])\n",
        "        else:  # Se la parola non è presente, applichiamo Laplace Smoothing\n",
        "            y_pos += math.log(1e-6)\n",
        "\n",
        "        if word in classifier_dict['P(w|neg)']:\n",
        "            y_neg += math.log(classifier_dict['P(w|neg)'][word])\n",
        "        else:\n",
        "            y_neg += math.log(1e-6)\n",
        "\n",
        "    # Restituiamo la classe con probabilità più alta\n",
        "    return \"Pos\" if y_pos > y_neg else \"Neg\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaMsFVeN-aAo"
      },
      "outputs": [],
      "source": [
        "def test_classifier(classifier, test_tweets, test_labels):\n",
        "    total = len(test_tweets)\n",
        "    correct = 0\n",
        "    for (tweet,label) in zip(test_tweets, test_labels):\n",
        "        predicted = tweet_classifier(tweet,classifier)\n",
        "        if predicted == label:\n",
        "            correct = correct + 1\n",
        "    return(correct/total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgyS5uis-aAo"
      },
      "outputs": [],
      "source": [
        "new_test_labels = test_labels.replace(0, \"Neg\", regex=True)\n",
        "final_test_labels = new_test_labels.replace(1, \"Pos\", regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iktPHfnw-aAo"
      },
      "source": [
        "This will take a while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY_J9q8N-aAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a33f939-b3e7-449c-f97b-533f1750b2a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4973\n"
          ]
        }
      ],
      "source": [
        "acc = test_classifier(classifier, test_tweets, final_test_labels)\n",
        "print(f\"Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJP6AARY-aAo"
      },
      "source": [
        "#### Optional work\n",
        "\n",
        "In basic sentiment analysis classifications we have 3 classes \"Positive\", \"Negative\" and \"Neutral\". Although because it is challenging to create the \"Neutral\" class. Try to improve the accuracy by filtering the dataset from the perspective of removing words that indicate neutrality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV2W07s1-aAo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}